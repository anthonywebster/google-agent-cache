# ğŸš€ Google Generative AI Chat API â€” Context Cache

[![Express](https://img.shields.io/badge/Express-5-black?logo=express)](#)
[![Google Generative AI](https://img.shields.io/badge/Google%20Generative%20AI-Gemini-4285F4?logo=google)](#)
[![License: ISC](https://img.shields.io/badge/License-ISC-blue.svg)](./LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](#)

Un servidor Express listo para chatear con Gemini usando Context Cache: sube tus fuentes una vez, crea un cachÃ© remoto y Ãºsalo como contexto base en tus conversaciones. âœ¨

---

## ğŸ“š Tabla de contenidos

- ğŸ§  QuÃ© es y cÃ³mo funciona
- âš™ï¸ InstalaciÃ³n rÃ¡pida
- ğŸ” ConfiguraciÃ³n (.env)
- ğŸ§ª Healthcheck
- ğŸ§© API (endpoints)
- ğŸ› ï¸ CLI (comandos)
- ğŸ“¦ Estructura sugerida
- ğŸ“˜ Ejemplos
- â“ FAQ y tips

---

## ğŸ§  QuÃ© es y cÃ³mo funciona

1. Entrena un Context Cache subiendo tus archivos. 2) El nombre del cachÃ© se guarda localmente. 3) Cada chat reutiliza ese contexto y puedes aÃ±adir contexto y archivos adicionales por request.

---

## âš™ï¸ InstalaciÃ³n rÃ¡pida

```bash
npm install
# Crea .env con tu GEMINI_API_KEY (o copia .env.example si existe)
# Coloca tus fuentes en data/cache_sources/
```

Arranque del servidor:

```bash
npm run start
```

Configurar el cachÃ© por CLI (opciones):

```bash
npm run setup-cache -- [sourcesDir] [displayName] [model] [ttlSeconds] [systemInstruction]
# Ejemplo:
npm run setup-cache -- data/cache_sources "Cache_Experto" models/gemini-2.5-pro 3600 "Eres experto en aduanas..."
```

---

## ğŸ” ConfiguraciÃ³n (.env)

- GEMINI_API_KEY=tu_api_key
- CACHE_NAME=tu_nombre_de_cache
- MODEL_NAME=models/gemini-2.5-pro
- PORT=3000

Modelos: usa uno que soporte createCachedContent (por ejemplo models/gemini-2.5-pro).

---

## ğŸ§ª Healthcheck

GET /health â†’ { status: 'ok' }

---

## ğŸ§© API (endpoints)

### 1) Context Cache

- POST /api/cache/setup
  - body: { filePath: string, mimeType: string, displayName?, model?, ttlSeconds?, systemInstruction?, cacheDisplayName? }
  - Sube el archivo, espera el procesamiento, crea el cachÃ© remoto y guarda el nombre en cache.json
- GET /api/cache â†’ InformaciÃ³n del cachÃ© guardado
- DELETE /api/cache â†’ Elimina la referencia local (no borra el cachÃ© remoto)

### 2) Chat

- POST /api/chat
  - body: { question: string, context?: string, files?: [{ path: string, mimeType: string, displayName?: string }] }
  - Usa el cachÃ© como contexto base + contexto/archivos opcionales
- POST /api/chat/upload (multipart/form-data)
  - fields: question (requerido), context (opcional)
  - files: mÃºltiples PDFs/TXT/MD en el campo files; se suben a Gemini y se aÃ±aden al prompt

---

## ğŸ› ï¸ CLI (comandos)

| Comando      | DescripciÃ³n                           | Uso                                                                            |
| ------------ | ------------------------------------- | ------------------------------------------------------------------------------ |
| list-models  | Lista modelos y mÃ©todos soportados    | `npm run list-models`                                                          |
| setup-cache  | Sube fuentes y crea un Context Cache  | `npm run setup-cache -- [dir] [displayName] [model] [ttl] [systemInstruction]` |
| list-caches  | Lista cachÃ©s remotos                  | `npm run list-caches`                                                          |
| delete-cache | Elimina un cachÃ© remoto por name o ID | `npm run delete-cache -- cachedContents/XXX` o `npm run delete-cache -- XXX`   |

Tip: usa `npm run list-caches` para copiar el campo name exacto.

---

## ğŸ“¦ Estructura sugerida

```
.
â”œâ”€ data/
â”‚  â””â”€ cache_sources/      # Tus fuentes (.pdf, .txt, .md, ...)
â”œâ”€ src/
â”‚  â”œâ”€ cli/                # Scripts CLI
â”‚  â””â”€ server.js           # Servidor Express
â”œâ”€ cache.json             # Nombre del cachÃ© guardado
â””â”€ .env                   # GEMINI_API_KEY
```

---

## ğŸ“˜ Ejemplos

JSON (ruta local en el servidor):

```bash
curl -X POST http://localhost:3000/api/chat \
  -H 'Content-Type: application/json' \
  -d '{
    "question": "Â¿QuÃ© documentos faltan?",
    "context": "Embarque MX-001",
    "files": [{
      "path": "data/cache_sources/mi-archivo.pdf",
      "mimeType": "application/pdf",
      "displayName": "mi-archivo.pdf"
    }]
  }'
```

Subiendo PDFs desde el cliente (multipart/form-data):

```bash
curl -X POST http://localhost:3000/api/chat/upload \
  -F "question=Â¿QuÃ© BL aplica?" \
  -F "context=Embarque MX-001" \
  -F "files=@data/cache_sources/ejemplo1.pdf;type=application/pdf" \
  -F "files=@data/cache_sources/ejemplo2.pdf;type=application/pdf"
```

---

## â“ FAQ y tips

- AsegÃºrate de que el modelo soporte createCachedContent.
- Si no tienes .env.example, crea .env y aÃ±ade GEMINI_API_KEY.
- Puedes recrear el cachÃ© cuando caduque usando la CLI.

---

Hecho con â¤ï¸ para desarrolladores que necesitan respuestas con contexto persistente. âœ¨
